---
title             : "Assignment Statistics 6"
shorttitle        : "Reproducibility Report"

author: 
  - name          : "Gustavo Villca Ponce"
    affiliation   : "1"
    corresponding : no
    email         : "gustavo.villcaponce@student.kuleuven.be"
  - name          : "MohammadHossein Haqiqatkhah"
    affiliation   : "2"
    corresponding : yes
    address       : "Psichological Institute, Tiensestraat 102, 3000 Leuven, Belgium"
    email         : "mh.haqiqatkhah@student.kuleuven.be"
  - name          : "Sigert Ariens"
    affiliation   : "3"
    corresponding : no
    email         : "sigert.ariens@studen.kuleuven.be"
    
  
affiliation:
  - id            : "1"
    institution   : "r0292033"
  - id            : "2"
    institution   : "r0607671"
  - id            : "3"
    institution   : "r0446864"
  - id            : ""
    institution   : "Faculty of Psychology and Educational Sciences, KU Leuven."
  
wordcount         : "1613"

bibliography      : ["r-references.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---
```{r load_packages and set-up, include = FALSE}

devtools::install_github("crsh/papaja")
list.of.packages <- c("papaja", "knitr", "haven", "gvlma", "tidyr", "Rmisc","lattice","tictoc", "gplots", "citr")

new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)){install.packages(new.packages,repos = "http://cran.us.r-project.org")}
lapply(list.of.packages, require, character.only = TRUE)

#knit2html("file.html")

knitr::opts_chunk$set( echo = FALSE, include = TRUE, error = FALSE, warning = FALSE, message = FALSE)
```
## Introducion 

This is a statistical report, based on @przybylski2017large for the class of _Statistics VI (Seminar on statistical analyses of psychological research data) [P0Q01a]_. As required by the guidelines of this project, this report will consist of three main parts, in which we will try to 1) Check the reproducibility status of the published results
, 2) Check the robustness status of the confirmatory analyses and 3) Check the pre-registration status of the published results by comparing the pre-registered protocol to the published paper. 
```{r loading-data, cache=TRUE}

from.csv <- read.csv("data.csv",header=T,sep=",")
from.sav <- read_sav("data.sav")

clean.csv <- as.data.frame(na.omit(from.csv))
clean.sav <- as.data.frame(na.omit(from.sav))

clean <- clean.csv

L <- list("watch_wd","watch_wd_sq","watch_we", "watch_we_sq", "play_wd", "play_wd_sq", "play_we", "play_we_sq", "comp_wd", "comp_wd_sq","comp_we", "comp_we_sq", "sp_wd", "sp_wd_sq", "sp_we", "sp_we_sq")

cnames<-colnames(clean)
```

```{r Functions for report}

model.maker <- function(data, variable.name, add.gender=F, add.ethnic=F, add.ses=F, agg.vars=F){
  #  L <- list("watch_wd","watch_wd_sq","watch_we", "watch_we_sq", "play_wd", "play_wd_sq", "play_we", "play_we_sq", "sp_wd", "sp_wd_sq", "sp_we", "sp_wd_sq", "comp_wd", "comp_wd_sq","comp_we","comp_we_sq")
  cnames <- colnames(data)
  name.no1 <- match(variable.name,cnames)
  name.no2 <- match(paste(variable.name,"_sq", sep=""),cnames)
  lin.comp <- as.numeric(unlist(data[,name.no1]))
  quad.comp <- as.numeric(unlist(data[,name.no2]))
  
  
  gender.comp <- data$male
  ethnic.comp <- data$minority
  ses.comp <- data$deprived
  
  if(agg.vars){
    gender.comp <- data$Genderg
    ethnic.comp <- data$Ethnicg
    ses.comp <- data$IMD3
  }
  
  model <- lm(data$mwb ~ lin.comp + quad.comp)
  if (add.gender==T & add.ethnic==T & add.ses==T){
    model <- lm(data$mwb ~ lin.comp + quad.comp + gender.comp + ethnic.comp + ses.comp)}
  if (add.gender==T & add.ethnic==T & add.ses==F){
    model <- lm(data$mwb ~ lin.comp + quad.comp + gender.comp + ethnic.comp)}
  if (add.gender==T & add.ethnic==F & add.ses==T){
    model <- lm(data$mwb ~ lin.comp + quad.comp + gender.comp + ses.comp)}
  if (add.gender==T & add.ethnic==F & add.ses==F){
    model <- lm(data$mwb ~ lin.comp + quad.comp + gender.comp)}
  if (add.gender==F & add.ethnic==T & add.ses==T){
    model <- lm(data$mwb ~ lin.comp + quad.comp + ethnic.comp + ses.comp)}
  if (add.gender==F & add.ethnic==T & add.ses==F){
    model <- lm(data$mwb ~ lin.comp + quad.comp + ethnic.comp)}
  if (add.gender==F & add.ethnic==F & add.ses==T){
    model <- lm(data$mwb ~ lin.comp + quad.comp + ses.comp)}
  
  return(model)
} #Fuction to make quadratic models

data <- c(1:12)
dimnames <- list(time=c("linear","quadratic"), name=c(1:6))
mat <- matrix(data, ncol=6, nrow=2, dimnames=dimnames)

row.maker <- function (model){
  
  table <- matrix(nrow = 2, ncol = 6)
  
  Columns <-c("b", "SE", "CI(2.5%)", "CI(97.5%)", "p", "d")
  colnames(table) <- Columns
  rownames(table) <- c("linear","quadratic")
  
  out <- summary(model)
  coefs <- coef(model)
  confints <- confint(model)
  
  df.sqrt <- sqrt(model$df)
  cohens.d.lin <- 2*abs(summary(model)$coefficients[2,3]/df.sqrt)
  cohens.d.quad <- 2*abs(summary(model)$coefficients[3,3]/df.sqrt)
  
  table[1,1] <- coefs[2]
  table[2,1] <- coefs[3]
  table[1, 2] <- out$coefficients[2,2]
  table[2,2] <- out$coefficients[3,2]
  table[1,3] <- confints[2,1] #2.5% part of linear model CI
  table[2,3] <- confints[3,1] #2,5% part of quadratic model CI
  table[1,4] <- confints[2,2] #97.5% part of linear model CI
  table[2,4] <- confints[3,2] #97.5% part of quadratic model CI
  table[1,5] <- out$coefficients[2,4] #p values
  table[2,5] <- out$coefficients[3,4]
  table[1,6] <- cohens.d.lin
  table[2,6] <- cohens.d.quad
  
  return(table)
  
}

table.maker <- function(data, add.gender=F, add.ethnic=F, add.ses=F, agg.vars=F, digits = 2){
  
  table <- NULL
  rows.tmp <- NULL
  l <- list("watch_wd","watch_we", "play_wd", "play_we", "comp_wd", "comp_we", "sp_wd", "sp_we")
  activities <- list("Watch Weekday", "Watch Weekend", "Play Weekday", "Play Weekend", "Computer Weekday", "Computer Weekend", "Smatphone Weekday", "Smatphone Weekend")
  
  for(i in 1:length(l)){
    rows.tmp <- round(row.maker(model.maker(data,l[i], add.gender=add.gender, add.ethnic=add.ethnic, add.ses=add.ses, agg.vars = agg.vars)),digits)
    table <- rbind(table, rows.tmp)
  }
  
  for(i in 1:length(activities)){
    rownames(table)[2*i-1]<-paste(activities[i],"Linear")
    rownames(table)[2*i]<-paste(activities[i],"Quadratic")
  }
  
  return(table)
  
} #Function to make tables with all the important statistics

recoder <- function(data, minority.code = c(0,1,1,1,1), depr.code = c(0,1,1)){
  
  # re-coding ethnicity
  data$minority[data$Ethnicg==1] <- minority.code[1] # Whites
  data$minority[data$Ethnicg==2] <- minority.code[2] # Mixed / multiple ethnic groups
  data$minority[data$Ethnicg==3] <- minority.code[3] # Asian / Asian British
  data$minority[data$Ethnicg==4] <- minority.code[4] # Black / African / Caribbean / Black British
  data$minority[data$Ethnicg==5] <- minority.code[5] # Other ethnic group
  
  # re-coding deprivation
  data$deprived[data$IMD3==1] <- depr.code[1] # lower IMD3 quantiles
  data$deprived[data$IMD3==2] <- depr.code[2] # middle IMD3 quantiles
  data$deprived[data$IMD3==3] <- depr.code[3] # higher IMD3 quantiles
  
  return(data)
  
}


concise <- function (model){
  
  p.values <- summary(model)$coefficients[,4]
  effect.sizes <- 2*abs(summary(model)$coefficients[,3]/sqrt(model$df))
  
  return(data.frame(p.values,effect.sizes))
  
} #Function for P-values and effect-size's


```


## Reproducibility

### Exploratory analysis

```{r BIC Analysis}
## Comparing
uncontrolled.models.linear <- list(NA)
uncontrolled.models.quad <- list(NA)
degree.model.uncontrolled <- list(NA)
calc.bic.unctrl <- data.frame(NA)

for (i in 1:(length(L)/2)){
  c3 <- match(L[2*i-1],cnames)
  c4 <- match(L[2*i],cnames)
  
  lin.comp <- as.numeric(unlist(clean[,c3]))
  quad.comp <- as.numeric(unlist(clean[,c4]))
  
  uncontrolled.models.linear[[i]] <- lm(clean$mwbi ~  lin.comp)
  uncontrolled.models.quad[[i]] <- lm(clean$mwbi ~ lin.comp + quad.comp)
  calc.bic.unctrl <- BIC(uncontrolled.models.linear[[i]], uncontrolled.models.quad[[i]])
  degree.model.uncontrolled[i] <- which.min(calc.bic.unctrl$BIC)
}

# Comparing controlled linear vs linear+quadratic models

controlled.models.linear <- list(NA)
controlled.models.quad <- list(NA)
degree.model.controlled <- list(NA)
calc.bic.ctrl <- data.frame(NA)

for (i in 1:(length(L)/2)){
  c3 <- match(L[2*i-1],cnames)
  c4 <- match(L[2*i],cnames)
  
  lin.comp <- as.numeric(unlist(clean[,c3]))
  quad.comp <- as.numeric(unlist(clean[,c4]))
  
  controlled.models.linear[[i]] <- lm(clean$mwbi ~  lin.comp + clean$male + clean$minority + clean$deprived)
  controlled.models.quad[[i]] <- lm(clean$mwbi ~ lin.comp + quad.comp + clean$male + clean$minority + clean$deprived)
  calc.bic.ctrl <- BIC(controlled.models.linear[[i]], controlled.models.quad[[i]])
  degree.model.controlled[i] <- which.min(calc.bic.ctrl$BIC)
}

```
To begin the replication portion of this report, we start by exploring the possibility of a monotonic relationship between digital screen-time and mental well-being  as described by @przybylski2017large, We achieve this by making use of the Bayesian Information Criterion (BIC) and comparing the simple linear models of all variables concerning digital screen-time with their simple and quadratic counterparts. We found that for the models that controlled for confounding variables, the fit was greater for the models with both a linear and quadratic component. For the unadjusted models, we found one model of which the BIC index favored a linear model alone. Specifically, for smartphone use during weekdays (see, @przybylski2017large, Figure 1).

### Confirmatory analysis

```{r table making }

table.uncontrolled <- table.maker(clean, add.gender=F, add.ethnic=F, add.ses=F, agg.vars = T, digits=2)
table.controlled <- table.maker(clean, add.gender=T, add.ethnic=T, add.ses=T, agg.vars = T, digits=2)
table.uncontrolled.kable <- kable(table.uncontrolled, caption = "Table.1 Unadjusted models" )

```

Following the steps described by the authors we start the exploratory data analysis by creating regression models of all four types of digital activities consisting of both linear and quadratic components, next we extracted all the important values (_$SD$_, _$p$_-values, _$\beta$_, confidence intervals, and _Cohen’s d_) out of the models and created two tables ,the first table contains the outcome of the models without taking into account the control variables described in the paper, namely gender, ethnicity and Socio-Economical Status (SES). The second table contains the outcomes of the models with the control variables (See tables below).

```{r, results = "asis"}
apa_table(x = table.uncontrolled, caption = "Unadjusted models", note = "This table contains statistics pertaining to our replication of the original adjusted models.")
apa_table(x = table.controlled, caption = "Adjusted models", note = "This table contains statistics pertaining to our replication of the original unadjusted models.")
```


```{r Examples}
example1 <- table.uncontrolled[1, 1]
example2 <- table.uncontrolled[5, 2]
example3 <- table.uncontrolled[5, 6]
example4 <- table.controlled[5, 1]
```

### Reproducibility analysis
Although we were able to extract all the important statistics from the raw data without too many issues, notice that some of our values are different from those reported in @przybylski2017large. Specifically, we noticed two types of differences in both tables , the first type are small one decimal differences, for example, in our replication of their analysis we obtain a $\beta$ value of $\beta =`r example1`$ for the linear component of the variable “watching films and tv programs in weekdays”, whereas in their paper the authors reports  a value of $\beta=0.98$, we see this occur not only for $\beta$ but for other values too, such as the standard deviation of the linear component of the variable “time spent playing games” (our $SD =`r example2`$ vs. their  $SD =0.11$), in the same model we obtain a Cohen's $d$ of $`r example3`$, compared to their $d$ value of $0.19$. These small one decimal differences can be found in both tables, a potential explanation would be a differences in the rounding of the number. This explanation becomes less likely, however, once we take onto account the second type of difference we encountered. We found decimal differences exceeding one decimal, for example, in table 2 we observe a $\beta =`r example4`$ for the linear component of “playing games in the weekdays” vs  $\beta =0.21$ reported in the paper. These multi-decimal differences can’t be explain fully by a rounding difference of the decimals. In order to find the origin of the different outputs, we looked into the data used by the authors for their SPSS analysis. We noticed a difference in the amount of missing values (NA) between the raw data and the data used in their SPSS analysis, which will be elaborated on in the preregistration section. 
It is likely that the researcher handled the missing values in a  way that wasn’t reported in the paper, making it hard for us to fully replicate the results without any differences. Furthermore, the way the variables were coded in an ambiguous manner, making it difficult to determine which specific variables were used in the models reported by the researchers. Overall, there was a lack of clarity in crucial data processing steps such as missing values and variable identification. 

``` {r Model-driven-multiverse, cache = TRUE, dependson="loading-data"}

# making difefrent combinations of models

model.combinations <- 8

data <- clean

model.combinations.p <- matrix(nrow = 16, ncol = model.combinations)
model.combinations.d <- matrix(nrow = 16, ncol = model.combinations)


for (i in 1:model.combinations){
  
  bit.code <- as.integer(intToBits(i-1))[1:3]
  this.data <- data
  model.tmp <- table.maker(data = this.data, add.gender = bit.code[1], add.ethnic = bit.code[2], add.ses = bit.code[3], digits = 9)
  model.combinations.p[,i] <- model.tmp[,5]
  model.combinations.d[,i] <- model.tmp[,6]
  
}
xlim.d <- c(0,1)
ylim.d <- c(0,50)

histogram.model.driven.1 <- histogram(model.combinations.p[5,], breaks = 10, xlim = xlim.d, ylim = ylim.d)
histogram.model.driven.2 <- histogram(model.combinations.p[13,], breaks = 10, xlim = xlim.d, ylim = ylim.d)
histogram.model.driven.3 <- histogram(model.combinations.p[14,], breaks = 10, xlim = xlim.d, ylim = ylim.d)

p.values.avg.5.nog <- mean(model.combinations.p[5,1],model.combinations.p[5,3],model.combinations.p[5,5],model.combinations.p[5,7])
p.values.avg.5.g <- mean(model.combinations.p[5,2], model.combinations.p[5,4],model.combinations.p[5,6],model.combinations.p[5,8])
p.values.avg.6 <- mean(model.combinations.p[6, ])

mean.p.model.driven.1 <- 2*mean(model.combinations.p[5,])
mean.p.model.driven.2 <- 2*mean(model.combinations.p[13,])
mean.p.model.driven.3 <- 2*mean(model.combinations.p[14,])


```


```{r Data-driven-multiverse, cache = TRUE, dependson="loading-data"}

# making different combinations of data

data.combinations <- 64

data <- clean

data.combinations.p <- matrix(nrow = 16, ncol = data.combinations)
data.combinations.d <- matrix(nrow = 16, ncol = data.combinations)

for (i in 1:data.combinations){
  
  bit.code <- as.integer(intToBits(i-1))[1:6]
  this.data <- recoder(data = data, minority.code = c(0,bit.code[1],bit.code[2],bit.code[3], bit.code[4]), depr.code = c(0,bit.code[5],bit.code[6]))
  model.tmp <- table.maker(data = this.data, add.gender = T, add.ethnic = T, add.ses = T, digits = 5)
  data.combinations.p[,i] <- model.tmp[,5]
  data.combinations.d[,i] <- model.tmp[,6]
  #print(i)
  }

# P values
xlim.d <- c(0,.15)
ylim.d <- c(0,25)

significant.p.data.driven.1 <- sum(data.combinations.p[5, ] <0.01)
significant.p.data.driven.2 <- sum(data.combinations.p[13, ] <0.01)

relative.frequency.data.driven.1 <- significant.p.data.driven.1/64
relative.frequency.data.driven.2 <- significant.p.data.driven.2/64

```
### Multiverse Analysis
#### Model Driven Multiverse
We found that, by manipulating the variables included in the models in 8 different ways (including/excluding each control variable "gender", "ethnicity", and "SES") this multiverse of models resulted in different _p_ values for only three component in two models: linear component of the weekday video game play, and both linear and quadratic components of smartphone use during the week. The only variable underlying this difference was "gender". Including the control variable of gender, regardless of any other combination of variables, shifted the average p value of the three mentioned components by `r mean.p.model.driven.1`, `r mean.p.model.driven.2`, and `r mean.p.model.driven.3`, respectively.  This finding is evident in Figure \@ref(fig:heat-p-model) and \@ref(fig:heat-d-model): the key role of gender as a control variables is reflected on the heat maps as the the odd and even columns are virtually the same.

#### Data Driven Multiverse

We took into account different combinations of possible coding strategies that could have been used by the researchers, in an approach akin to @steegen2016increasing. For the recoding of the categorical variable "ethnicity", we allowed each level but "white" to be coded into the binary variable "minority". The same was done for the categorical variable "deprived", although similar to the previous coding, the first level was maintained due to reasonability concerns. Gender (which was coded into "male" in the original paper) was already a binary variable hence was not considered in the data driven multiverse. This resulted in a multiverse consisting of $2^6 = 64$ possible codings of the variable. We then examined the distribution of the _p_ and _d_ values resulting from applying the full model, thus including all control variables, to the different datasets.

Two models had diverse distributions of _p_ values; Specifically, linear component of the regression model for variables of game play and smartphone usage in the weekdays. The distributions of _p_ values for these models are ploted in Figure \@ref(fig:plotpfirst) and \@ref(fig:plotpsecond) respectively. Please note that the plots' axes scales defer in the figures. Also note that although the _p_ values are correctly shown in the figures by vertical lines, the curves do not represent the 'actual' probability distribution of the _p_ values; they are rather an approximate for the corresponding histograms. For a more clear insight into the significance levels and effect sizes across the data driven multiverse, see the heat maps in Figure \@ref(fig:heat-p-data) and \@ref(fig:heat-d-data). Figure \@ref(fig:heat-p-data) confirm our observation about relatively lower significance of two regression models across the data driven multiverse. On top of that, Figure \@ref(fig:heat-d-data) (together with Figure \@ref(fig:heat-p-data)) roughly suggests that different coding combinations for minority and deprivation status do not influence the significance and effect size dramatically. This is in line with the outcome of the model driven multiverse analysis where we observed that the decision about controling for gender has a key influence on the regression models.

```{r, plotpfirst, fig.cap = "Distribution of p values for different combinations of data for the linear component of the regression model of game play on the weekdays. Note that the curve does not represent the true probability distribution of the p values.", fig.align = "center"}
ddd1 <- density(data.combinations.p[5,])
ydd <- rep(0,64)+0*rnorm(64,0,0.75)
{plot(ddd1$x,100*ddd1$y/64, type="l", xlab = "p values", ylab = "Estimated percent of p values", main = "Distribution of p values for game play.")
points(data.combinations.p[5,],ydd, col="blue", type = "p", pch = "|")}
```
```{r, plotpsecond, fig.cap = "Distribution of p values for different combinations of data for the linear component of the regression model of smartphone usage on weekdays. Note that the curve does not represent the true probability distribution of the p values.", fig.align = "center"}
ddd2 <- density(data.combinations.p[13,])
ydd <- rep(0,64)+0*rnorm(64,0,0.75)
{plot(ddd2$x,100*ddd2$y/length(ddd2$y), type="l", xlab = "p values", ylab = "Estimated percent of p values", main = "Distribution of P values for smartphone usage")
points(data.combinations.p[13,],ydd, col="blue", type = "p", pch = "|")
}
```

```{r, heat-p-model, fig.cap = "Heat map of p values for different decisions about inclusion/exclusion of control variables in the regression models. Rows represents the coresponding rows in Table 1 and 2 and each column is for one of the 8 different inclusion/exclusion combinations.", fig.width=8}

mm <- (as.matrix(model.combinations.p))
heatmap.2(x = mm, Rowv = FALSE, Colv = FALSE, dendrogram = "none",
          key.par=list(mar=c(3.5,1,3,0)),col = heat.colors(10),
          trace = "none", key = TRUE, keysize = 0.5, density.info='histogram',
          denscol="blue", ylab = "Different models",
          xlab = "Different choices of control variables",
          lmat=rbind(c(5,4,2), c(6,1,3)), lhei=c(2.5, 5), lwid=c(1, 10, 1))

```

```{r, heat-d-model, fig.cap = "Heat map of Cohen's d for different decisions about inclusion/exclusion of control variables in the regression models. Rows represents the coresponding rows in Table 1 and 2 and each column is for one of the 8 different inclusion/exclusion combinations.", fig.width=8}

mm <- (as.matrix(model.combinations.d))
heatmap.2(x = mm, Rowv = FALSE, Colv = FALSE, dendrogram = "none",
          key.par=list(mar=c(3.5,1,3,0)),col = heat.colors(50),
          trace = "none", key = TRUE, keysize = 0.5, density.info='histogram',
          denscol="blue", ylab = "Different models",
          xlab = "Different choices of control variables",
          lmat=rbind(c(5,4,2), c(6,1,3)), lhei=c(2.5, 5), lwid=c(1, 10, 1))

```

```{r, heat-p-data, fig.cap = "Heat map of p values for different choices of variable coding. Rows represents the coresponding rows in Table 1 and 2 and each column is for one of the 64 different data combinations.", fig.width=8}

mm <- (as.matrix(data.combinations.p))
heatmap.2(x = mm, Rowv = FALSE, Colv = FALSE, dendrogram = "none",
          key.par=list(mar=c(3.5,1,3,0)),col = heat.colors(10),
          trace = "none", key = TRUE, keysize = 0.5, density.info='histogram',
          denscol="blue", ylab = "Different models", xlab = "Different data combinations",
          lmat=rbind(c(5,4,2), c(6,1,3)), lhei=c(2.5, 5), lwid=c(1, 10, 1))

```

```{r, heat-d-data, fig.cap = "Heat map of Cohen's d for different choices of variable coding. Rows represents the corresponding rows in Table 1 and 2 and each column is for one of the 64 different data combinations.", fig.width=8}

mm <- (as.matrix(data.combinations.d))
heatmap.2(x = mm, Rowv = FALSE, Colv = FALSE, dendrogram = "none",
          key.par=list(mar=c(3.5,1,3,0)),col = heat.colors(50),
          trace = "none", key = TRUE, keysize = 0.5, density.info='histogram',
          denscol="blue", ylab = "Different models", xlab = "Different data combinations",
          lmat=rbind(c(5,4,2), c(6,1,3)), lhei=c(2.5, 5), lwid=c(1, 10, 1))

```

Interestingly, we find that _p_ values corresponding to the linear component of the regression model for weekday smartphone use is significant (for a significance threshold of $\alpha = 0.01$) only for $`r signif(relative.frequency.data.driven.2,2)`\%$ of the multiverse encapsulating all possible researcher choices in variable coding. This percent drops to $`r signif(relative.frequency.data.driven.1,2)`\%$ for the linear component of weekday game play. It is worth mentioning that for all other models we do not observe any noticable change in the _p_ values. Please note that the significance threshold was originally suggested as $\alpha = 0.001$ in the paper. However, since this conservative threshold resulted in zero multiverse-frequencies, we increased it to $\alpha = 0.01$. These differences, along with the difference in regression parameters reported above, can likely be ascribed to the ambiguities mentioned throughout the paper.

### Preregistration

The data were acquired according to the specifications made by the authors in the preregistration document. However, in the technical report on their OSF page, the authors said to use a 3% margin of error at the 95%CI to estimate sample size. In the published paper, the authors report a 0.3% margin of error, arriving at the same estimate of sample size (N = 298,080. Furthermore, The authors reported a total n of 120,115 participants with usable data. When we attempted to replicate their analyses, we met with a further reduction of n to `r nrow(clean.csv)`. This is not reported anywhere in the published article. Finally, the two data documents provided by the authors differ in the amount of NA data they contain. Where the .csv file contains `r nrow(from.csv)-nrow(clean.csv)` rows with missing values, the .sav file contains `r nrow(from.sav)-nrow(clean.sav)` rows with missing values. None of these inconsistencies were reported by the authors in the final paper, and it is unclear which dataset the authors ultimately used in their analysis. The fact that the data are ambiguous is a major obstacle for replication analyses.

The preregistration document stated that testing the displacement hypothesis was to be done by linear regression modeling, predicting mental well-being through composite scores of screen time. The authors did not conduct these analyses. They explain that ‘Interocular’ tests were sufficient to exclude this hypothesis. Although we only found one unadjusted model for which the BIC criterium favors a purely linear model, this does allow one to question why the authors refrained from the formal hypothesis test detailed in the preregistration document. 

The authors ignored the measure of summed screen time which they included in their preregistration document. The authors reported this accordingly, although our above analysis allows the questionability of their deviation on this point.
Finally, the authors did not concretely specify what particular coding they intended to use for the control variables of ‘whether living in a deprived area’, and ‘whether black and minority ethnicity’. While the conditionality of the term ‘whether’ implies binary coding, the subsequent reference to the specific questions would also allow one to assume that the authors used the values from those questions. This is relevant because other codings of these variables are also present in the data. 
Although this is a minor issue, a more clear issue is also present. The authors noted in their deviations from analysis plan that the preregistered control variables of ‘whether parents married’ and ‘whether native-born’, they did not include the omission of these variables in the published article. 


\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup