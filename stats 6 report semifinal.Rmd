---
title             : "Assigment Statistics 6"
shorttitle        : "Reproducibility Report"

author: 
  - name          : "Gustavo Villca Ponce"
    affiliation   : "1"
    corresponding : no    # Define only one corresponding author
    email         : "gustavo.villcaponce@student.kuleuven.be"
  - name          : "MohammadHossein Haqiqatkhah"
    affiliation   : "2"
    corresponding : no
    email         : "mh.haqiqatkhah@student.kuleuven.be"
  - name          : "Sigert Ariens"
    affiliation   : "3"
    corresponding : no
    email         : "sigert.ariens@studen.kuleuven.be"
    
  
affiliation:
  - id            : "1"
    institution   : "student number:r0292033"
  - id            : "2"
    institution   : "student number:r0607671"
  - id            : "3"
    institution   : "student number:"

  
wordcount         : "X"

bibliography      : ["r-references.bib"]

figsintext        : yes
figurelist        : no
tablelist         : yes
footnotelist      : no
lineno            : no
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---
```{r load_packages and set-up, include = FALSE}

devtools::install_github("crsh/papaja")
list.of.packages <- c("papaja", "knitr", "haven", "gvlma", "tidyr", "Rmisc","lattice","tictoc")

new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)){install.packages(new.packages,repos = "http://cran.us.r-project.org")}
lapply(list.of.packages, require, character.only = TRUE)

#knit2html("file.html")

knitr::opts_chunk$set( echo = F , include = T, error = F, warning = F, message = F)
```
## Introducion 

This is a statistical report, based on (add citation) for the class of _Statistics VI (Seminar on statistical analyses of psychological research data) [P0Q01a]_. As required by the guidelines of this project, this report will consist of three main parts, in which we will try to 1)Check the reproducibility status of the published results
, 2)Check the robustness status of the confirmatory analyses and 3) Check the pre-registration status of the published results by comparing the pre-registered protocol to the published paper. 
```{r loding_data}

from.csv <- read.csv("data.csv",header=T,sep=",")
from.sav <- read_sav("data.sav")

clean.csv <- as.data.frame(na.omit(from.csv))
clean.sav <- as.data.frame(na.omit(from.sav))

clean <- clean.csv

L <- list("watch_wd","watch_wd_sq","watch_we", "watch_we_sq", "play_wd", "play_wd_sq", "play_we", "play_we_sq", "comp_wd", "comp_wd_sq","comp_we", "comp_we_sq", "sp_wd", "sp_wd_sq", "sp_we", "sp_we_sq")

cnames<-colnames(clean)
```

```{r Fuctions for report}

model.maker <- function(data, variable.name, add.gender=F, add.ethnic=F, add.ses=F, agg.vars=F){
  #  L <- list("watch_wd","watch_wd_sq","watch_we", "watch_we_sq", "play_wd", "play_wd_sq", "play_we", "play_we_sq", "sp_wd", "sp_wd_sq", "sp_we", "sp_wd_sq", "comp_wd", "comp_wd_sq","comp_we","comp_we_sq")
  cnames <- colnames(data)
  name.no1 <- match(variable.name,cnames)
  name.no2 <- match(paste(variable.name,"_sq", sep=""),cnames)
  lin.comp <- as.numeric(unlist(data[,name.no1]))
  quad.comp <- as.numeric(unlist(data[,name.no2]))
  
  
  gender.comp <- data$male
  ethnic.comp <- data$minority
  ses.comp <- data$deprived
  
  if(agg.vars){
    gender.comp <- data$Genderg
    ethnic.comp <- data$Ethnicg
    ses.comp <- data$IMD3
  }
  
  model <- lm(data$mwb ~ lin.comp + quad.comp)
  if (add.gender==T & add.ethnic==T & add.ses==T){
    model <- lm(data$mwb ~ lin.comp + quad.comp + gender.comp + ethnic.comp + ses.comp)}
  if (add.gender==T & add.ethnic==T & add.ses==F){
    model <- lm(data$mwb ~ lin.comp + quad.comp + gender.comp + ethnic.comp)}
  if (add.gender==T & add.ethnic==F & add.ses==T){
    model <- lm(data$mwb ~ lin.comp + quad.comp + gender.comp + ses.comp)}
  if (add.gender==T & add.ethnic==F & add.ses==F){
    model <- lm(data$mwb ~ lin.comp + quad.comp + gender.comp)}
  if (add.gender==F & add.ethnic==T & add.ses==T){
    model <- lm(data$mwb ~ lin.comp + quad.comp + ethnic.comp + ses.comp)}
  if (add.gender==F & add.ethnic==T & add.ses==F){
    model <- lm(data$mwb ~ lin.comp + quad.comp + ethnic.comp)}
  if (add.gender==F & add.ethnic==F & add.ses==T){
    model <- lm(data$mwb ~ lin.comp + quad.comp + ses.comp)}
  
  return(model)
} #Fuction to make quadratic models

data <- c(1:12)
dimnames <- list(time=c("linear","quadratic"), name=c(1:6))
mat <- matrix(data, ncol=6, nrow=2, dimnames=dimnames)

row.maker <- function (model){
  
  table <- matrix(nrow = 2, ncol = 6)
  
  Columns <-c("b", "SE", "CI(2.5%)", "CI(97.5%)", "p", "d")
  colnames(table) <- Columns
  rownames(table) <- c("linear","quadratic")
  
  out <- summary(model)
  coefs <- coef(model)
  confints <- confint(model)
  
  df.sqrt <- sqrt(model$df)
  cohens.d.lin <- 2*abs(summary(model)$coefficients[2,3]/df.sqrt)
  cohens.d.quad <- 2*abs(summary(model)$coefficients[3,3]/df.sqrt)
  
  table[1,1] <- coefs[2]
  table[2,1] <- coefs[3]
  table[1, 2] <- out$coefficients[2,2]
  table[2,2] <- out$coefficients[3,2]
  table[1,3] <- confints[2,1] #2.5% part of linear model CI
  table[2,3] <- confints[3,1] #2,5% part of quadratic model CI
  table[1,4] <- confints[2,2] #97.5% part of linear model CI
  table[2,4] <- confints[3,2] #97.5% part of quadratic model CI
  table[1,5] <- out$coefficients[2,4] #p values, should be made nicer by replacing small values with "<.005)
  table[2,5] <- out$coefficients[3,4]
  table[1,6] <- cohens.d.lin
  table[2,6] <- cohens.d.quad
  
  return(table)
  
}

table.maker <- function(data, add.gender=F, add.ethnic=F, add.ses=F, agg.vars=F, digits = 2){
  
  table <- NULL
  rows.tmp <- NULL
  l <- list("watch_wd","watch_we", "play_wd", "play_we", "comp_wd", "comp_we", "sp_wd", "sp_we")
  activities <- list("Watch Weekday", "Watch Weekend", "Play Weekday", "Play Weekend", "Computer Weekday", "Computer Weekend", "Smatphone Weekday", "Smatphone Weekend")
  
  for(i in 1:length(l)){
    rows.tmp <- round(row.maker(model.maker(data,l[i], add.gender=add.gender, add.ethnic=add.ethnic, add.ses=add.ses, agg.vars = agg.vars)),digits)
    table <- rbind(table, rows.tmp)
  }
  
  for(i in 1:length(activities)){
    rownames(table)[2*i-1]<-paste(activities[i],"Linear")
    rownames(table)[2*i]<-paste(activities[i],"Quadratic")
  }
  
  return(table)
  
} #Function to make tables with all the important statistics

recoder <- function(data, minority.code = c(0,1,1,1,1), depr.code = c(0,1,1)){
  
  # re-coding ethnicity
  data$minority[data$Ethnicg==1] <- minority.code[1] # Whites
  data$minority[data$Ethnicg==2] <- minority.code[2] # Mixed / multiple ethnic groups
  data$minority[data$Ethnicg==3] <- minority.code[3] # Asian / Asian British
  data$minority[data$Ethnicg==4] <- minority.code[4] # Black / African / Caribbean / Black British
  data$minority[data$Ethnicg==5] <- minority.code[5] # Other ethnic group
  
  # re-coding deprivation
  data$deprived[data$IMD3==1] <- depr.code[1] # lower IMD3 quantiles
  data$deprived[data$IMD3==2] <- depr.code[2] # middle IMD3 quantiles
  data$deprived[data$IMD3==3] <- depr.code[3] # higher IMD3 quantiles
  
  return(data)
  
}


concise <- function (model){
  
  p.values <- summary(model)$coefficients[,4]
  effect.sizes <- 2*abs(summary(model)$coefficients[,3]/sqrt(model$df))
  
  return(data.frame(p.values,effect.sizes))
  
} #Function for P-values and effect-size's


```


## Reproducibility

### Exploratory analysis

```{r BIC Analysis}
## Comparing
uncontrolled.models.linear <- list(NA)
uncontrolled.models.quad <- list(NA)
degree.model.uncontrolled <- list(NA)
calc.bic.unctrl <- data.frame(NA)

for (i in 1:(length(L)/2)){
  #i<-4
  c3 <- match(L[2*i-1],cnames)
  c4 <- match(L[2*i],cnames)
  
  lin.comp <- as.numeric(unlist(clean[,c3]))
  quad.comp <- as.numeric(unlist(clean[,c4]))
  
  uncontrolled.models.linear[[i]] <- lm(clean$mwbi ~  lin.comp)
  uncontrolled.models.quad[[i]] <- lm(clean$mwbi ~ lin.comp + quad.comp)
  calc.bic.unctrl <- BIC(uncontrolled.models.linear[[i]], uncontrolled.models.quad[[i]])
  degree.model.uncontrolled[i] <- which.min(calc.bic.unctrl$BIC)
}

#Comparing controlled linear vs linear+quadratic models

controlled.models.linear <- list(NA)
controlled.models.quad <- list(NA)
degree.model.controlled <- list(NA)
calc.bic.ctrl <- data.frame(NA)

for (i in 1:(length(L)/2)){
  #i<-4
  c3 <- match(L[2*i-1],cnames)
  c4 <- match(L[2*i],cnames)
  
  lin.comp <- as.numeric(unlist(clean[,c3]))
  quad.comp <- as.numeric(unlist(clean[,c4]))
  
  controlled.models.linear[[i]] <- lm(clean$mwbi ~  lin.comp + clean$male + clean$minority + clean$deprived)
  controlled.models.quad[[i]] <- lm(clean$mwbi ~ lin.comp + quad.comp + clean$male + clean$minority + clean$deprived)
  calc.bic.ctrl <- BIC(controlled.models.linear[[i]], controlled.models.quad[[i]])
  degree.model.controlled[i] <- which.min(calc.bic.ctrl$BIC)
}



```
To begin the replication portion of this report, we start by exploring the possibility of a monotonic relationship between digital screen-time and mental well-being  as described by Przybylski and Weinstein (2017), We achieve this my making use of the Besyan Information Criterion (BIC) and comparing the simple linear models of all variables concerning digital screen-time with their simple and quadratic counterparts. We found that for the models that controlled for confounding variables, the fit was greater for the models with both a linear and quadratic component. For the unadjusted models, we found one model of which the BIC index favoured a linear model alone. Specifically, for smartphone use during weekdays ( see, Przybylski & Weinstein (2017), figure 1).

### Confirmatory analysis 
```{r table making }

table.uncontrolled <- table.maker(clean, add.gender=F, add.ethnic=F, add.ses=F, agg.vars = T, digits=2)
table.controlled <- table.maker(clean, add.gender=T, add.ethnic=T, add.ses=T, agg.vars = T, digits=2)
table.uncontrolled.kable <- kable(table.uncontrolled, caption = "Table.1 Uncontrolled variables" )

```

Following the steps described by the authors we start the exploratory data analysis by creating quadratic models of all four types of digital activities consisting of both linear and of non-linear components, next we extracted all the important value( _SD_, _P_-values, _β_, Confidence intervals and Cohen’s _d_)    out of the models and created two tables , the first table contains the outcome of the models without taking into account the control variables described in the paper, namely gender ,ethnicity and Socio Economical Status. The second table contains the outcomes of the models with the control variables (See tables below)

`r apa_table(x = table.controlled, caption = "Controlled variables")`
`r kable(x = table.uncontrolled, caption = "Uncontrolled variables")`
`r print(kable(x = table.uncontrolled, caption = "Uncontrolled variables"))`


```{r Examples }
example1 <- table.uncontrolled[1, 1]
example2 <- table.uncontrolled[5, 2]
example3 <- table.uncontrolled[5, 6]
example4 <- table.controlled[5, 1]
```

### Reproducibility analysis
Although we were able to extract all the important statistics from the raw data without too many issues, notice that some of our values are different from those reported in Przybylski and Weinstein (2017). Specifically, we noticed two types of differences in both tables , the first type are small one decimal differences, for example, in our replication of their analysis we obtain a β value of $\beta$ =`r example1` for the linear component of the variable “watching films and tv programs in weekdays”, whereas in their paper the authors reports  a value of $\beta$=.98, we see this occur not only for β but for other values too, such as the standard deviation of the linear component of the variable “time spent playing games” (our $SD$ =`r example2` vs their  $SD =.11$ ), in the same model we obtain a $cohens'd$ value of `r example3`, compared to their $d$ value of 19. These small one decimal differences can be found in both tables, a potential explanation would be a differences in the rounding of the number. This explanation becomes less likely, however, once we take onto account the second type of difference we encountered. We found decimal differences exceeding one decimal, for example, in table 2 we observe a $\beta$ =`r example4` for the linear component of “playing games in the weekdays” vs  $\beta$ =.21 reported in the paper. These multi-decimal differences can’t be explain fully by a rounding difference of the decimals. In order to find the origin of the different outputs, we looked into the data used by the authors for their SSPS analysis. We noticed a difference in the amount of missing values(NA) between the raw data and the data used in their SSPS analysis, which will be elaborated on in the preregistration section. 
It is likely that the researcher handled the missing values in a  way that wasn’t reported in the paper, making it hard for us to fully replicate the results without any differences. Furthermore, the way the variables were coded in an ambiguous manner, making it difficult to determine which specific variables were used in the models reported by the researchers. Overall, there was a lack of clarity in crucial data processing steps such as missing values and variable identification. 

``` {r Model_driven_multiverse, cache = TRUE}

#  To use higher ordder linear models
#  fit.quad <- gvlma(weight~poly(dayTime,2,raw=TRUE), data=new.data)

b <- as.integer(intToBits(13))


cc <- recoder(clean, minority.code=c(b[1],b[2],b[3],b[4],b[5]))

# making difefrent combinations of models

model.combinations <- 8

data <- clean

model.combinations.p <- matrix(nrow = 16, ncol = model.combinations)
model.combinations.d <- matrix(nrow = 16, ncol = model.combinations)


for (i in 1:model.combinations){
  
  bit.code <- as.integer(intToBits(i-1))[1:3]
  this.data <- data
  model.tmp <- table.maker(data = this.data, add.gender = bit.code[1], add.ethnic = bit.code[2], add.ses = bit.code[3], digits = 9)
  model.combinations.p[,i] <- model.tmp[,5]
  model.combinations.d[,i] <- model.tmp[,6]
  
}
xlim.d <- c(0,1)
ylim.d <- c(0,50)

histogram(model.combinations.p[5,], breaks = 10, xlim = xlim.d, ylim = ylim.d)
histogram(model.combinations.p[6,], breaks = 10, xlim = xlim.d, ylim = ylim.d)

p.values.avg.5.nog <- mean(model.combinations.p[5,1],model.combinations.p[5,3],model.combinations.p[5,5],model.combinations.p[5,7])
p.values.avg.5.g <- mean(model.combinations.p[5,2], model.combinations.p[5,4],model.combinations.p[5,6],model.combinations.p[5,8])
p.values.avg.6 <- mean(model.combinations.p[6, ])



```


```{r Data_driven_multiverse, cache=T}
#  To use higher ordder linear models
#  fit.quad <- gvlma(weight~poly(dayTime,2,raw=TRUE), data=new.data)

b <- as.integer(intToBits(13))

# making difefrent combinations of data

data.combinations <- 64

data <- clean

data.combinations.p <- matrix(nrow = 16, ncol = data.combinations)
data.combinations.d <- matrix(nrow = 16, ncol = data.combinations)

# Uncomment these lines after you have run it once, and you want to work on the code, otherwise you'll have to wait a while
mytic <- tic()
for (i in 1:data.combinations){
  
  bit.code <- as.integer(intToBits(i-1))[1:6]
  this.data <- recoder(data = data, minority.code = c(0,bit.code[1],bit.code[2],bit.code[3], bit.code[4]), depr.code = c(0,bit.code[5],bit.code[6]))
  model.tmp <- table.maker(data = this.data, add.gender = T, add.ethnic = T, add.ses = T, digits = 5)
  data.combinations.p[,i] <- model.tmp[,5]
  data.combinations.d[,i] <- model.tmp[,6]
  print(i)
}
mytac <- toc()


xlim.d <- c(0,1)
ylim.d <- c(0,50)

Hist.datadriven.5 <- histogram(data.combinations.p[5,], breaks = 10, xlim = xlim.d, ylim = ylim.d)
Hist.datadriven.6 <- histogram(data.combinations.p[6,], breaks = 10, xlim = xlim.d, ylim = ylim.d)

p.values.5 <- sum(data.combinations.p[5, ] <0.01)
p.values.6 <- sum(data.combinations.p[6, ] <0.01)

relative.frequency <- p.values.6/64
#relative.frequency

#library(DataExplorer)
#create_report(clean)

```
### Multiverse Analysis
#### Model Driven Multiverse
We found that, by manipulating the variables included in the models in 8 different ways (including/excluding each control variable "gender", "ethnicity", and "SES". We found that this multiverse of models only resulted in different p values for the linear and linear+quadratic models of weekday game usage, and the only variable underlying this difference was "gender". Including the control variable of gender, regardless of any other combination of variables, shifted the average p value of the slope of the linear model from p = `r p.values.avg.5.nog` to `p.values.avg.5.g`.

#### Data Driven Multiverse

We took into account different combinations of possible coding strategies that could have been used by the researchers, in an approach akin to Steegen, Tuerlinckx, Gelman & Vanpaemel (2016). For the recoding of the categorical variable "ethnicity", we allowed each level but "white" to be recoded into the binary variable "minority". The same was done for the categorical variable "deprived", although similarly to the previous recoding, the first level was maintained due to reasonability concerns. This resulted in a multiverse consisting of $2^6 = 64$ possible codings of the variable. We then examined the distribution of the p and d values resulting from applying the full model, thus including all control variables, to the different data sets.
Two models had diverse distributions of p values. Specifically, the distributions of p values of the purely linear model of weekday game usage resulted in the histogram in figure 1
`r Hist.datadriven.5`
The distributions of p values of the linear+quadratic model of weekday game usage resulted in the histogram in figure 2
`r Hist.datadriven.6`
It is interesting to see the multiverse-frequency of p values below the significance threshold of $\alpha =$ .001 given by the researchers. We find that, for this model, the multiverse encapsulating all possible researcher choices in variable coding only results in a significant value for this model in `r signif(relative.frequency,2)`% of the cases. For all other cases, the p values of the models seem reasonable. However, we obtain a different p value than the authors when using the full model on the subset of the data multiverse used by the authors. Specifically, we find a p value of p = `r data.combinations.p[5,1]` for the linear model describing weekday game usage, compaired to the $p = 0.059$ authors. These differences, along with the difference in regression parameters reported above, can likely be ascribed to the ambiguities mentioned throughout the paper. 



### Preregistration

The data were acquired according to the specifications made by the authors in the preregistration document. However, in the technical report on their OSF page, the authors said to use a 3% margin of error at the 95%CI to estimate sample size. In the published paper, the authors report a 0.3% margin of error, arriving at the same estimate of sample size (N = 298,080. Furthermore, The authors reported a total n of 120,115 participants with usable data. When we attempted to replicate their analyses, we met with a further reduction of n to `r nrow(clean.csv)`. This is not reported anywhere in the published article. Finally, the two data documents provided by the authors differ in the amount of NA data they contain. Where the .csv file contains `r nrow(from.csv)-nrow(clean.csv)` rows with missing values, the .sav file contains `r nrow(from.sav)-nrow(clean.sav)` rows with missing values. None of these inconsistencies were reported by the authors in the final paper, and it is unclear which data set the authors ultimately used in their analysis. The fact that the data are ambiguous is a major obstacle for replication analyses.

The preregistration document stated that testing the displacement hypothesis was to be done by linear regression modelling, predicting mental well-being through composite scores of screen time. The authors did not conduct these analyses. They explain that ‘Interocular’ tests were sufficient to exclude this hypothesis. Although we only found one unadjusted model for which the BIC criterum favours a purely linear model, this does allow one to question why the authors refrained from the formal hypothesis test detailed in the preregistration document. 

The authors ignored the measure of summed screen time which they included in their preregistration document. The authors reported this accordingly, although our above analysis allows the questionability of their deviation on this point.
Finally, the authors did not concretely specify what particular coding they intended to use for the control variables of ‘whether living in a deprived area’, and ‘whether black and minority ethnicity’. While the conditionality of the term ‘whether’ implies binary coding, the subsequent reference to the specific questions would also allow one to assume that the authors used the values from those questions. This is relevant because other codings of these variables are also present in the data. 
Although this is a minor issue, a more clear issue is also present. The authors noted in their deviations from analysis plan that the preregistered control variables of ‘whether parents married’ and ‘whether native born’, they did not include the omission of these variables in the published article. 


\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup